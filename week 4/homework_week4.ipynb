{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc1cc33-0457-484e-b3a0-021a90fce663",
   "metadata": {},
   "source": [
    "## Homework week 4 - Evaluation and Monitoring\n",
    "In this homework, we'll evaluate the quality of our RAG system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f838d-b570-407d-86de-9439a696fa45",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "Let's start by getting the dataset. We will use the data we generated in the module.\n",
    "\n",
    "In particular, we'll evaluate the quality of our RAG system with [gpt-4o-mini](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/04-monitoring/homework.md#:~:text=Let%27s%20start%20by,Read%20it%3A)\n",
    "\n",
    "Read it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f20fe20e-0417-42aa-a5a5-d3b85e95e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999c251-85c5-429d-9e10-5d78729a2ee7",
   "metadata": {},
   "source": [
    "We will use only the first 300 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "628fe358-a4f5-44b4-8b73-c5aa481e4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92283422-450e-4bcc-8b01-d0d4f2a426aa",
   "metadata": {},
   "source": [
    "### Q1. Getting the embeddings model\n",
    "Now, get the embeddings model multi-qa-mpnet-base-dot-v1 from the [Sentence Transformer library](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/04-monitoring/homework.md#:~:text=Now%2C%20get%20the%20embeddings%20model%20multi%2Dqa%2Dmpnet%2Dbase%2Ddot%2Dv1%20from%20the%20Sentence%20Transformer%20library)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa9dfed4-a431-49a5-97ed-6d21f19f349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1a2ff-72a9-4817-974e-65be461a8370",
   "metadata": {},
   "source": [
    "Create the embeddings for the first LLM answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df67ed68-7899-4e9c-8894-f9f1a0c86df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.42244655)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "v = embedding_model.encode(answer_llm)[0]\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b066f-d1fd-45e5-b294-4ba1ad65fa5c",
   "metadata": {},
   "source": [
    "### Q2. Computing the dot product\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the evaluations list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd63df91-152d-4c68-8d37-024333090d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).',\n",
       " 'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       " 'document': '0227b872',\n",
       " 'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = df.to_dict(orient='records')\n",
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec20f9b3-bc0c-46ce-917b-3a2eecaee340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 300/300 [01:25<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for doc in tqdm(records):\n",
    "    v_llm = embedding_model.encode(doc['answer_llm'])\n",
    "    v_orig = embedding_model.encode(doc['answer_orig'])\n",
    "    evaluations.append(v_llm.dot(v_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0755a766-1903-40fe-9358-d899cac8e44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(31.674309)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.percentile(evaluations, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a21658-3dae-49e8-9f96-dc982f81227d",
   "metadata": {},
   "source": [
    "### Q3. Computing the cosine\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we\n",
    "\n",
    "- Compute the norm of a vector\n",
    "- Divide each element by this norm\n",
    "So, for vector v, it'll be v / ||v||\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10a440a6-c370-4bd9-b6cb-c20b5e1ee125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    return  v / np.sqrt((v * v).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "534a5d41-c104-4f0f-a0e1-1c9b2c6036e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 300/300 [01:24<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "norm_evaluations = []\n",
    "\n",
    "for doc in tqdm(records):\n",
    "    v_llm = embedding_model.encode(doc['answer_llm'])\n",
    "    v_orig = embedding_model.encode(doc['answer_orig'])\n",
    "    llm_norm = normalize(v_llm)\n",
    "    orig_norm = normalize(v_orig)\n",
    "    norm_evaluations.append(llm_norm.dot(orig_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "393aaac1-d3df-4326-81d8-a6da3ab4a833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728393\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651273\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = pd.Series(norm_evaluations)\n",
    "evaluations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44182297-a9c2-434e-ab31-885bc5276172",
   "metadata": {},
   "source": [
    "### Q4. Rouge\n",
    "Now we will explore an alternative metric - the ROUGE score.\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b751c43-9b33-41c5-8ef4-ab5a683c698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d69d5-b951-4e10-8129-2ef2dac20183",
   "metadata": {},
   "source": [
    "(The latest version at the moment of writing is 1.0.1)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (doc_id=5170565b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11ad738f-ed2c-4fa8-b7fd-4de5000b5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "doc = records[10]\n",
    "scores = rouge_scorer.get_scores(doc['answer_llm'], doc['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d318dc8a-aaa6-4cca-b1e7-421c17a0a689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d13c02-1eb4-448b-9c7e-10a27675bbd3",
   "metadata": {},
   "source": [
    "### Q5. Average rouge score\n",
    "Let's compute the average F-score between rouge-1, rouge-2 and rouge-l for the same record from Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0fb67863-77fe-474f-bbf3-8b84cfeee098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35490034990035496"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "(rouge_1 + rouge_2 + rouge_l ) /3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e59a8d-7516-4104-a977-fb4b6dff4f82",
   "metadata": {},
   "source": [
    "### Q6. Average rouge score for all the data points\n",
    "Now let's compute the F-score for all the records and create a dataframe from them.\n",
    "\n",
    "What's the average F-score in rouge_2 across all the records?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ba74bc2-b7e1-489f-a015-97e0cde44aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 300/300 [00:00<00:00, 411.98it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for doc in tqdm(records):\n",
    "    scores = rouge_scorer.get_scores(doc['answer_llm'], doc['answer_orig'])[0]\n",
    "\n",
    "    rouge_1 = scores['rouge-1']['f']\n",
    "    rouge_2 = scores['rouge-2']['f']\n",
    "    rouge_l = scores['rouge-l']['f']\n",
    "    \n",
    "    evaluations.append({\n",
    "        'rouge_1':rouge_1,\n",
    "        'rouge_2':rouge_2,\n",
    "        'rouge_l':rouge_l,\n",
    "        'rouge_avg':(rouge_1 + rouge_2 + rouge_l ) /3,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b81877d7-c000-4b52-a845-3d86a86cb8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.206965\n",
       "std        0.153550\n",
       "min        0.000000\n",
       "25%        0.097809\n",
       "50%        0.178671\n",
       "75%        0.286181\n",
       "max        0.739130\n",
       "Name: rouge_2, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = pd.DataFrame(evaluations)\n",
    "evaluation['rouge_2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b87abc-5ccb-4365-ab44-91a723af2d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
